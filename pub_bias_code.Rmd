---
title: "Test, adjust for, and report publication bias: a primer"
subtitle: "Doing publication bias test in R: a Hands-on guide"
author: "Yefeng Yang, Malgorzata Lagisz, Alfredo Sánchez-Tójar"
output:
  rmdformats::robobook:
    code_folding: show
    self_contained: true
    thumbnails: false
    lightbox: true
pkgdown:
  as_is: true   
bibliography: "ref/REFERENCES.bib"
csl: "ref/Ecology.csl"
link-citations: yes
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Instructions  
This interactive format Rmarkdown/html is used to provide participants with a hands-on guide and R code, which can be directly adjusted to their own publication bias tests.


# Loading packages  

Load the`R` packages used in this tutorial. The main `R` packages used in this tutorial are `metafor` (for model fitting; @viechtbauer2010conducting) and `orchaRd` (for results reporting; @nakagawa2021orchard). All other `R` packages are used for data manipulations, visualizations and rmarkdown knitting.   

```{r, cache = FALSE}
# the following packages are used for knitting, participants do not need to install and load them
pacman::p_load(knitr,
               DT,
               readxl,
               patchwork,
               ggplot2,
               pander,
               formatR,
               rmdformats
               )
```


Below we randomly choose one meta-analysis paper in @costello2022decline's dataset as an example to show how to properly test, adjust for and report publication bias.  

> Montagano L, Leroux S J, Giroux M A, et al. The strength of ecological subsidies across ecosystems: a latitudinal gradient of direct and indirect impacts on food webs[J]. Ecology Letters, 2019, 22(2): 265-274.

# Load and process data from the example paper  

```{r read data}
# load package
library(readr) # package for reading data 
library(tidyverse) # package for data manipulation
# load data
montagano.et.al.2019.ecol.letts.dat <- read_csv(file = "data/montagano.et.al.2019.ecol.letts.csv")

# have a look at the data
head(montagano.et.al.2019.ecol.letts.dat)
```


As shown in the slides, the extended Egger's regression model looks complex, but it is not difficult to construct it using existing software, for example, `rma.mv()` function in `metafor` package.

```{r caclulate effect size}
# load package
library(metafor) # package for performing meta-analysis
# compute effect size and its sampling variance
montagano.et.al.2019.ecol.letts.dat <- escalc(measure = "ROM", 
                                              n1i = n_subsidy, n2i = n_nosubsidy,
                                              m1i = Subsidy_mean, m2i = Nosubsidy_mean, 
                                              sd1i = Subsidy_sd, sd2i = Nosubsidy_sd, 
                                              append=TRUE, replace=TRUE, 
                                              data = montagano.et.al.2019.ecol.letts.dat)

# delete NAs
montagano.et.al.2019.ecol.letts.dat <- montagano.et.al.2019.ecol.letts.dat[!is.na(montagano.et.al.2019.ecol.letts.dat$yi) & !is.na(montagano.et.al.2019.ecol.letts.dat$vi), ]

# have a look at the data
t <- montagano.et.al.2019.ecol.letts.dat %>% DT::datatable()
t
```


# Test publication bias (testing small-study effects and decline effects)    

Recall the mathematical formulas used to test publication bias:

$$
y_{i} = \beta_{0} + \beta_{1}se_{i} + \beta_{2}c(year_{j}) + \sum \beta_{k}x_{k} + \mu_{(b)j} + \mu_{(w)i} + m_{i}, \\ \mu_{(b)j} \sim N(0,\sigma_{(b)}^2)\\ \mu_{(w)i} \sim N(0,\sigma_{(w)}^2)\\ m_{i} \sim N(0,\nu_{i})
$$

```{r test pb}
# add an unique identifier to each row (effect size) to account for multiple effect sizes nested within the same studies
montagano.et.al.2019.ecol.letts.dat$obsID <- 1:nrow(montagano.et.al.2019.ecol.letts.dat)

# center publication year and other continuous moderator variables to ease interpretation of the results
## center year
montagano.et.al.2019.ecol.letts.dat$Year.c <- montagano.et.al.2019.ecol.letts.dat$Year - mean(montagano.et.al.2019.ecol.letts.dat$Year) # this variable will be used as a predictor to estimate decline effects
## center latitude
montagano.et.al.2019.ecol.letts.dat$Latitude.c <- montagano.et.al.2019.ecol.letts.dat$Latitude - mean(montagano.et.al.2019.ecol.letts.dat$Latitude)
## center altitude
montagano.et.al.2019.ecol.letts.dat$Longitude.c <- montagano.et.al.2019.ecol.letts.dat$Longitude - mean(montagano.et.al.2019.ecol.letts.dat$Longitude)

# create a variable to contain sampling error, which can be used as a predictor to identify small-study effect
montagano.et.al.2019.ecol.letts.dat$sei <- sqrt(montagano.et.al.2019.ecol.letts.dat$vi)

# simultaneously detect small-study effect and decline effect
pub_bias_test <- rma.mv(yi, vi, 
                        random = list(~ 1 | ID, ~ 1 | obsID),
                        mods = ~ sei + Year.c +  Latitude.c + Longitude.c + System + Taxon + Taxon -1,
                        test = "t",
                        method = "REML",
                        data = montagano.et.al.2019.ecol.letts.dat) # Latitude,  Longitude,  System, Taxon and Taxon are moderator variables used in the original paper
```

Let's have a look at the model results of the above fitted multilevel multi-moderator meta-regression with:

```{r}
summary(pub_bias_test)
```

# Report publication bias

We see that the model slope $\beta_{2}$ (`Year.c`) is not statistically significant, which indicates that there is no decline effect. Lets's visually present the results of the decline effect test (which can be done by `bubble_plot` in `orchaRd`; @nakagawa2021orchard)

```{r decline effect test, fig.cap="Figure 1. Bubble plot showing the relationship between publication year and effect size magnitude "}

# load package
library(orchaRd) # package for visualizations of results of publication bias test 
# you need to install the latest version via devtools::install_github("daniel1noble/orchaRd", force = TRUE, build_vignettes = TRUE)

de <- bubble_plot(pub_bias_test, mod = "Year.c", 
                  xlab = "Publication year (centered by mean year 2010)", ylab = "Effect size",
                  group = "ID",
                  data = montagano.et.al.2019.ecol.letts.dat, legend.pos = "none") +
                  theme(axis.text.x = element_text(size = 10, colour = "black"),
                        axis.text.y = element_text(size = 10, colour = "black"),
                        axis.title.x = element_text(size = 10, colour = "black"),
                        plot.title = element_text(size = 10, colour = "black"))
de
```

We can see a temporal trend in the changes of the effect size magnitude (**Figure 1**). The estimate of slope $\beta_{2}$ is -0.0245 (95% CI = -0.0521 to 0.0031), indicating that the effect size magnitude will decline by 0.245 over 10 years. If we only focus on the dichotomous reports on the decline effect test (*p*-value < 0.05 meaning decline effect *vs.* *p*-value > 0.05 meaning no decline effect), we would neglect the real decline of the effect size magnitude.


The similar interpretation philosophy can be applied to the test of small-study effect. The model slope $\beta_{1}$ (`sei`) is statistically significant, which indicates that there is a small-study effect. 

```{r small-study effect, fig.cap="Figure 2. Bubble plot showing the relationship between sampling error (inverse precision) anf effect size magnitude"}
sse <- bubble_plot(pub_bias_test, mod = "sei", 
                   xlab = "Sampling error", ylab = "Effect size",
                   group = "ID",
                   data = montagano.et.al.2019.ecol.letts.dat, legend.pos = "none") +
                   theme(axis.text.x = element_text(size = 10, colour = "black"),
                         axis.text.y = element_text(size = 10, colour = "black"),
                         axis.title.x = element_text(size = 10, colour = "black"),
                         plot.title = element_text(size = 10, colour = "black"))
sse
```

The visual presentation of the model results also confirms that small studies (with low precision) are more likely to report large effects (**Figure 2**). The estimate of slope $\beta_{2}$ is very large 0.6292 (95% CI = 0.23 to 1.0283), implying that we should interpret the results and conclusions of @montagano2019strength with caution.  

We can also use a funnel plot (Figure 3) to visually check the asymmetry.

```{r funnel plot, fig.cap="Figure 3. Visual inspection of the funnel plot to identify the small-study effect"}
# make a funnel plot
funnel(pub_bias_test, yaxis = "seinv", 
       ylab = "Precision (1/SE)",
       xlab = "Effect size estimates (lnRR)")
```

# Adjust for publication bias

Recall the mathematical formulas used to adjust for publication bias:

$$
y_{i} = \beta_{0} + \beta_{1}se_{i}^2 + \beta_{2}c(year_{j}) + \sum \beta_{k}x_{k} + \mu_{(b)j} + \mu_{(w)i} + m_{i}, \\ \mu_{(b)j} \sim N(0,\sigma_{(b)}^2)\\ \mu_{(w)i} \sim N(0,\sigma_{(w)}^2)\\ m_{i} \sim N(0,\nu_{i})
$$
```{r adjust for pb}
# fit the above formula
pub_bias_adjust <- rma.mv(yi, vi, 
                          random = list(~ 1 | ID, ~ 1 | obsID),
                          mods = ~ vi + Year.c, # do not remove intercept via "-1", because intercept is our focal model estimate
                          test = "t",
                          method = "REML",
                          data = montagano.et.al.2019.ecol.letts.dat)
```

Let's have a look at the model results of the above fitted multilevel multi-moderator meta-regression with:

```{r}
summary(pub_bias_adjust)
```

We can see that the adjusted effect size is statistically significant ($\beta_{0}$ = 0.5323, 95%CI = [0.3736, 0.6909], p-value < 0.0001). By comparing the original effect size and the adjusted effect size (see below table), we know that although this meta-analysis has publication bias, the result is robust.

```{r robustness}
# fit a intercept-only model to estimate the overall effect/pooled effect size
overall_effect_mod <- rma.mv(yi, vi, 
                             random = list(~ 1 | ID, ~ 1 | obsID),
                             test = "t",
                             method = "REML",
                             data = montagano.et.al.2019.ecol.letts.dat)

# make a table to compare the original effect size and the adjusted effect size
t1 <- data.frame("Overall effect (pooled lnRR)" = c(round(overall_effect_mod$b[1],4), round(pub_bias_adjust$b[1],4)),
             "Standard error" = c(round(overall_effect_mod$se,4), round(pub_bias_adjust$se[1],4)),
             "p-value" = c(round(overall_effect_mod$pval,4), round(pub_bias_adjust$pval[1],4)),
             "Lower CI" = c(round(overall_effect_mod$ci.lb,4), round(pub_bias_adjust$ci.lb[1],4)),
             "Upper CI" = c(round(overall_effect_mod$ci.ub,4), round(pub_bias_adjust$ci.ub[1],4)))

colnames(t1) <- c("Overall effect (pooled lnRR)", "Standard error", "p-value", "Lower CI", "Upper CI")

t1_2 <- t(t1) %>% as.data.frame()
colnames(t1_2) <- c("Original estimate", "Bias-corrected version")

t1_2 %>% DT::datatable()
```


# Conduct your own analyses and further materials to learn

Participants can modify the above code to test and adjust for publication bias (small-study effect and decline effect) and (visually) report the associated results. If you prefer an `rmd` version of the code, please refer to our [Github repository: Yefeng0920/Publication_bias_workshop_SORTEE2022](https://github.com/Yefeng0920/Publication_bias_workshop_SORTEE2022). If you are after a comprehensive theoretical knowledge on the publication bias test in ecology and evolution, we recommend you to thoroughly read our recent methodological paper published in *Methods in Ecology and Evolution* (@nakagawa2022methods). In this paper, we summarized the current practice of publication bias tests in ecology and evolution; we also proposed the multilevel multi-moderator meta-regression method approach.   

# References
